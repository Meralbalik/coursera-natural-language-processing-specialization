# Coursera Natural Language Processing Specialization

This repository contains material related to Coursera [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing). It consists of a bunch of Jupyter notebooks for each weekly assignment.

## Content

### Natural Language Processing with Classification and Vector Spaces

* [Sentiment Analysis with Logistic Regression](https://github.com/naderabdalghani/coursera-natural-language-processing-specialization/tree/main/1.%20Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week1): Learn to extract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression.
* [Sentiment Analysis with Na√Øve Bayes](https://github.com/naderabdalghani/coursera-natural-language-processing-specialization/tree/main/1.%20Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week2): Learn the theory behind Bayes' rule for conditional probabilities, then apply it toward building a Naive Bayes tweet classifier.
* [Vector Space Models](https://github.com/naderabdalghani/coursera-natural-language-processing-specialization/tree/main/1.%20Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week3): Vector space models capture semantic meaning and relationships between words. Learn how to create word vectors that capture dependencies between words, then visualize their relationships in two dimensions using PCA.
* [Machine Translation and Document Search](https://github.com/naderabdalghani/coursera-natural-language-processing-specialization/tree/main/1.%20Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week4): Learn to transform word vectors and assign them to subsets using locality sensitive hashing, in order to perform machine translation and document search.

### Natural Language Processing with Probabilistic Models

* [Autocorrect](https://github.com/naderabdalghani/coursera-natural-language-processing-specialization/tree/main/2.%20Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week1): Learn about autocorrect, minimum edit distance, and dynamic programming, then build a spellchecker to correct misspelled words.
* [Part of Speech Tagging and Hidden Markov Models](https://github.com/naderabdalghani/coursera-natural-language-processing-specialization/tree/main/2.%20Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week2): Learn about Markov chains and Hidden Markov models, then use them to create part-of-speech tags for a Wall Street Journal text corpus.
* [Autocomplete and Language Models](https://github.com/naderabdalghani/coursera-natural-language-processing-specialization/tree/main/2.%20Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week3): Learn about how N-gram language models work by calculating sequence probabilities, then build an autocomplete language model using a text corpus from Twitter.
* [Word Embeddings with Neural Networks](https://github.com/naderabdalghani/coursera-natural-language-processing-specialization/tree/main/2.%20Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week4): Learn about how word embeddings carry the semantic meaning of words, which makes them much more powerful for NLP tasks, then build a Continuous bag-of-words model to create word embeddings from Shakespeare text.

### Natural Language Processing with Sequence Models

* [Neural Networks for Sentiment Analysis](https://github.com/naderabdalghani/coursera-natural-language-processing-specialization/tree/main/3.%20Natural%20Language%20Processing%20with%20Sequence%20Models/Week1): Learn about neural networks for deep learning, then build a sophisticated tweet classifier that places tweets into positive or negative sentiment categories, using a deep neural network.
* [Recurrent Neural Networks for Language Modeling](https://github.com/naderabdalghani/coursera-natural-language-processing-specialization/tree/main/3.%20Natural%20Language%20Processing%20with%20Sequence%20Models/Week2): Learn about the limitations of traditional language models and see how RNNs and GRUs use sequential data for text prediction. Then build a next-word generator using a simple RNN on Shakespeare text data.
* [LSTMs and Named Entity Recognition](https://github.com/naderabdalghani/coursera-natural-language-processing-specialization/tree/main/3.%20Natural%20Language%20Processing%20with%20Sequence%20Models/Week3): Learn about how long short-term memory units (LSTMs) solve the vanishing gradient problem, and how Named Entity Recognition systems quickly extract important information from text. Then build a Named Entity Recognition system using an LSTM and data from Kaggle.
* [Siamese Networks](https://github.com/naderabdalghani/coursera-natural-language-processing-specialization/tree/main/3.%20Natural%20Language%20Processing%20with%20Sequence%20Models/Week4): Learn about Siamese networks, a special type of neural network made of two identical networks that are eventually merged together, then build a Siamese network that identifies question duplicates in a dataset from Quora.

### Natural Language Processing with Attention Models
